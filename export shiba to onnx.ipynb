{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5006fc-7a0c-4d15-a31e-5c235c40998c",
   "metadata": {},
   "source": [
    "# Export SHIBA model to ONNX\n",
    "Original SHIBA cannot be directly exported to ONNX format due to unsupported ONNX operation sets. See the change details in README.md.\n",
    "\n",
    "This notebook does the followings.\n",
    "1. Load SHIBA python (implemented with pytorch) and enable evaluation mode\n",
    "2. Export to ONNX format and specify input/output shapes\n",
    "3. Load the export model with onnxruntime, perform inference\n",
    "4. Compare the results of original and onnx models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c3e62",
   "metadata": {},
   "source": [
    "## Note:\n",
    "Make sure that `shiba` is imported from current directory as a local package, not from Python's site-packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8063e34-5afd-451e-9b84-2adf49dc539e",
   "metadata": {},
   "source": [
    "## 1. Load SHIBA python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc41c16-6d75-42a7-ad31-c0685b590994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from shiba import Shiba, CodepointTokenizer, get_pretrained_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff1cfb4c-340e-4c5d-a0aa-52e4197db249",
   "metadata": {},
   "outputs": [],
   "source": [
    "shiba_model = Shiba()\n",
    "shiba_model.load_state_dict(get_pretrained_state_dict())\n",
    "shiba_model.eval() # disable dropout\n",
    "tokenizer = CodepointTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7250eea0-a641-467d-aa2c-e6a90922345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_batch(['自然言語処理', '柴ドリル', '吾輩は猫である', '戻れないよ昔のようには'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e547fb-ff1e-4994-98ae-01b4193c563e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c94528-0fd3-4238-b9a1-3dde7b149aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[57344, 33258, 28982, 35328, 35486, 20966, 29702,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [57344, 26612, 12489, 12522, 12523,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [57344, 21566, 36649, 12399, 29483, 12391, 12354, 12427,     0,     0,\n",
       "              0,     0],\n",
       "         [57344, 25147, 12428, 12394, 12356, 12424, 26132, 12398, 12424, 12358,\n",
       "          12395, 12399]]),\n",
       " torch.Size([4, 12]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'], inputs['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51c07e1-4177-41c8-b759-06b0e8ea8100",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 12, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-8.4970e-02,  3.1598e-01,  4.6064e-01,  ..., -1.5094e-01,\n",
       "          -8.7310e-02, -5.1852e-01],\n",
       "         [ 1.4574e-02,  4.7369e-02, -5.1739e-02,  ..., -2.7652e-01,\n",
       "           3.0437e-01, -5.5507e-02],\n",
       "         [-5.0915e-01, -4.2272e-01, -3.5427e-01,  ..., -3.7649e-01,\n",
       "           7.0191e-01, -2.4434e-01],\n",
       "         ...,\n",
       "         [-5.2718e-01,  1.8932e-01, -1.8683e-01,  ..., -1.1507e+00,\n",
       "           1.5632e+00, -6.0920e-01],\n",
       "         [-5.8127e-01,  3.1314e-01, -3.4824e-01,  ..., -1.0217e+00,\n",
       "           2.8069e+00, -4.7585e-01],\n",
       "         [-1.3637e-01,  3.8597e-01, -4.8575e-01,  ..., -8.0985e-01,\n",
       "           6.8822e-01, -5.5001e-01]],\n",
       "\n",
       "        [[-1.0938e-01,  5.3974e-01,  2.6640e-01,  ..., -2.0817e-01,\n",
       "          -5.2853e-01, -5.8613e-01],\n",
       "         [ 6.7832e-02,  1.0756e-01, -1.2797e+00,  ..., -4.0049e-01,\n",
       "          -3.5392e-01, -1.8826e-01],\n",
       "         [-1.0239e+00, -7.9504e-01, -2.1978e-01,  ..., -1.0396e-01,\n",
       "           7.4251e-01,  2.7360e-01],\n",
       "         ...,\n",
       "         [-8.2568e-01, -1.1136e-01, -1.8044e+00,  ..., -3.0615e-01,\n",
       "           1.1698e+00, -6.1222e-01],\n",
       "         [-1.0123e+00, -1.4209e-01, -1.6507e+00,  ...,  1.6299e-01,\n",
       "           2.5652e+00,  2.0751e-01],\n",
       "         [-7.2552e-01, -1.3446e-01, -1.2435e+00,  ..., -2.8556e-01,\n",
       "           1.1071e+00, -3.6562e-01]],\n",
       "\n",
       "        [[ 3.3203e-03,  3.7083e-01,  7.2404e-02,  ..., -1.6630e-01,\n",
       "          -1.2932e-01, -2.5305e-01],\n",
       "         [-4.0025e-01,  6.1654e-01, -6.6117e-01,  ..., -3.2733e-01,\n",
       "           9.0110e-01, -4.2216e-01],\n",
       "         [ 3.4028e-01,  3.0928e-01, -2.5345e-01,  ..., -1.1365e-01,\n",
       "           6.3451e-01, -4.4574e-01],\n",
       "         ...,\n",
       "         [ 1.1786e-01, -3.7329e-01, -4.5521e-01,  ..., -6.5382e-01,\n",
       "           2.0458e+00, -4.3171e-01],\n",
       "         [-2.4047e-01, -3.7067e-01, -3.6526e-01,  ..., -7.7474e-01,\n",
       "           2.2999e+00, -1.8125e-01],\n",
       "         [ 8.8264e-02, -2.4736e-01, -5.1926e-01,  ..., -5.8348e-01,\n",
       "           6.9984e-01, -4.5676e-01]],\n",
       "\n",
       "        [[-4.7234e-01,  4.1501e-01,  2.3020e-01,  ...,  1.4014e-01,\n",
       "          -1.9397e-01, -4.1952e-01],\n",
       "         [ 3.9804e-01, -9.4476e-01,  1.5053e-01,  ..., -2.1131e-01,\n",
       "          -5.6980e-01, -4.2508e-01],\n",
       "         [ 7.4608e-01, -5.0023e-01, -6.0058e-01,  ...,  2.0135e-01,\n",
       "           7.7839e-01, -3.4769e-01],\n",
       "         ...,\n",
       "         [-3.2208e-01, -1.1754e-01, -3.0124e-01,  ...,  1.7836e-01,\n",
       "          -8.3577e-01, -3.5441e-01],\n",
       "         [-8.6243e-02,  4.5682e-02,  5.9426e-02,  ..., -1.7719e-01,\n",
       "          -6.3414e-02,  1.9999e-03],\n",
       "         [-2.6311e-01, -2.8238e-01, -7.2905e-02,  ..., -9.5215e-02,\n",
       "          -3.7517e-01,  1.5672e-02]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del inputs['attention_mask']\n",
    "\n",
    "embs = shiba_model(**inputs)['embeddings']\n",
    "print(\"Output shape:\", embs.size())\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f5a94e-5d01-4499-80a4-2532b54f9746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0466,  0.0478,  0.2602,  ...,  0.1234, -0.3078, -0.0344],\n",
       "         [-0.0549,  0.0487,  0.0168,  ..., -0.1427,  0.2436, -0.0129]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_one_char = tokenizer.encode_batch(['草'])\n",
    "del inputs_one_char['attention_mask']\n",
    "\n",
    "shiba_model(**inputs_one_char)['embeddings']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec61afd-7642-433d-8bf5-5866a09c9b60",
   "metadata": {},
   "source": [
    "## 2. Exporting to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a639781-4b28-4d9a-8834-eaa40dfaa448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c9bed16-ac26-4693-aeca-45063d1c979d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noppayus/shiba-onnx/shiba/local_attention/local_attention.py:46: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if is_integer:\n",
      "/home/noppayus/shiba-onnx/shiba/local_attention/local_attention.py:48: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  remainder = math.ceil(seqlen / multiple) * multiple - seqlen\n",
      "/home/noppayus/shiba-onnx/shiba/model.py:269: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  total_padding = torch.tensor(l * s - l + d * k - d + 1 - s)\n",
      "/home/noppayus/shiba-onnx/shiba/model.py:269: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  total_padding = torch.tensor(l * s - l + d * k - d + 1 - s)\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "onnx_export_name = \"shiba.onnx\"\n",
    "\n",
    "if \"attention_mask\" in inputs:\n",
    "    del inputs[\"attention_mask\"]\n",
    "    \n",
    "torch.onnx.export(shiba_model, \n",
    "                  inputs, \n",
    "                  onnx_export_name, \n",
    "                  verbose=False, \n",
    "                  input_names= [\"input_ids\"], \n",
    "                  output_names=[\"embeddings\"],\n",
    "                  dynamic_axes = {\n",
    "                      \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                      \"embeddings\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "                  },\n",
    "                  opset_version=13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b5e5f18-4326-44a1-97e6-915e509e5686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly tell shapes to prevent incorrect shape inference\n",
    "import onnx\n",
    "from onnx.tools import update_model_dims\n",
    "\n",
    "model = onnx.load(onnx_export_name)\n",
    "hidden_size = shiba_model.config.hidden_size\n",
    "fixed_out_dim_model = update_model_dims.update_inputs_outputs_dims(model, {\"input_ids\": [\"batch_size\", \"sequence_length\"]}, {\"embeddings\": [\"batch_size\", \"sequence_length\", hidden_size]})\n",
    "onnx.save(fixed_out_dim_model, onnx_export_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6535b203-982f-4bc2-bcd8-13d446a9c34a",
   "metadata": {},
   "source": [
    "## 3. Load the ONNX model and perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3da75591-1fb0-4270-9299-1626fcb2d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"shiba.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f5dcd09-614b-4d12-a7d9-2c1699ac273a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 21:15:11.100324660 [W:onnxruntime:, graph.cc:3494 CleanUnusedInitializersAndNodeArgs] Removing initializer '242'. It is not used by any node and should be removed from the model.\n",
      "2022-10-19 21:15:11.100383611 [W:onnxruntime:, graph.cc:3494 CleanUnusedInitializersAndNodeArgs] Removing initializer '233'. It is not used by any node and should be removed from the model.\n",
      "2022-10-19 21:15:11.100388609 [W:onnxruntime:, graph.cc:3494 CleanUnusedInitializersAndNodeArgs] Removing initializer '240'. It is not used by any node and should be removed from the model.\n",
      "2022-10-19 21:15:11.100393847 [W:onnxruntime:, graph.cc:3494 CleanUnusedInitializersAndNodeArgs] Removing initializer '234'. It is not used by any node and should be removed from the model.\n",
      "2022-10-19 21:15:11.100396414 [W:onnxruntime:, graph.cc:3494 CleanUnusedInitializersAndNodeArgs] Removing initializer '236'. It is not used by any node and should be removed from the model.\n",
      "2022-10-19 21:15:11.100398871 [W:onnxruntime:, graph.cc:3494 CleanUnusedInitializersAndNodeArgs] Removing initializer '229'. It is not used by any node and should be removed from the model.\n",
      "2022-10-19 21:15:11.100401262 [W:onnxruntime:, graph.cc:3494 CleanUnusedInitializersAndNodeArgs] Removing initializer '241'. It is not used by any node and should be removed from the model.\n",
      "2022-10-19 21:15:11.100403563 [W:onnxruntime:, graph.cc:3494 CleanUnusedInitializersAndNodeArgs] Removing initializer '232'. It is not used by any node and should be removed from the model.\n",
      "2022-10-19 21:15:11.100405942 [W:onnxruntime:, graph.cc:3494 CleanUnusedInitializersAndNodeArgs] Removing initializer '235'. It is not used by any node and should be removed from the model.\n",
      "2022-10-19 21:15:11.100408388 [W:onnxruntime:, graph.cc:3494 CleanUnusedInitializersAndNodeArgs] Removing initializer '238'. It is not used by any node and should be removed from the model.\n",
      "2022-10-19 21:15:11.100411295 [W:onnxruntime:, graph.cc:3494 CleanUnusedInitializersAndNodeArgs] Removing initializer '239'. It is not used by any node and should be removed from the model.\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_export_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebe7c6b1-6448-4b1a-a7e0-ae302b4813f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 12, 768)\n"
     ]
    }
   ],
   "source": [
    "inputs_np = {\n",
    "    'input_ids': inputs['input_ids'].numpy(),\n",
    "}\n",
    "\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    inputs_np,\n",
    ")\n",
    "print(outputs[0].shape)\n",
    "embs_np = outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326f3ab-e04e-49c5-9b8c-f3540e4936e4",
   "metadata": {},
   "source": [
    "## 4. Compare original and ONNX results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57e2a4fe-00c9-47d4-8552-f718fb3e61cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff larger than 0.0001: 0 / 36864 (0.00)%\n",
      "Diff larger than 1e-05: 702 / 36864 (1.90)%\n"
     ]
    }
   ],
   "source": [
    "for tol in (1e-4, 1e-5):\n",
    "    diff_pos = ~np.isclose(embs_np, embs.detach().numpy(), atol=tol)\n",
    "    n_diff = embs_np[diff_pos].size\n",
    "    n_all = embs_np.size\n",
    "    print(f\"Diff larger than {tol}: {n_diff} / {n_all} ({100*n_diff / n_all:.2f})%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3777d-620e-45ff-8957-937073ec9371",
   "metadata": {},
   "source": [
    "### Try other inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57344ac3-de6c-48db-b32f-a6dc6d48dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test = tokenizer.encode_batch(['沈むように溶けてゆくように', '今日は天気がいいから散歩しましょう！', '君がいなくなった日々もこの同省もない気だるさも'])\n",
    "del inputs_test[\"attention_mask\"]\n",
    "\n",
    "inputs_test_np = {\n",
    "    'input_ids': inputs_test['input_ids'].numpy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb445da7-eeb3-4950-832d-66afcb81aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = shiba_model(**inputs_test)['embeddings']\n",
    "embs_np = ort_session.run(None, inputs_test_np)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b0cab3d-c584-4326-a3f9-3a31c8d145c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff larger than 0.0001: 1 / 55296 (0.00)%\n",
      "Diff larger than 1e-05: 484 / 55296 (0.88)%\n"
     ]
    }
   ],
   "source": [
    "for tol in (1e-4, 1e-5):\n",
    "    diff_pos = ~np.isclose(embs_np, embs.detach().numpy(), atol=tol)\n",
    "    n_diff = embs_np[diff_pos].size\n",
    "    n_all = embs_np.size\n",
    "    print(f\"Diff larger than {tol}: {n_diff} / {n_all} ({100*n_diff / n_all:.2f})%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7b159-7c5e-4d1e-ac79-84255377fd54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.14 64-bit ('3.8.14')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "550f3431236b0eae805fd4f1c6a0ba93d45d93d2f6732156e563b3f672c6a040"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
